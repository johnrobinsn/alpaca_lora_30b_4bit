{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca (LLama) Lora-4bit quantized weights\n",
    "This is an inference only notebook for trying out my Alpaca trained lora adapter (4bit)\n",
    "\n",
    "The adapter took a little over 5 days to train on a single Titan RTX (24G) GPU.\n",
    "\n",
    "Notes:\n",
    "* if you have more than one GPU; might need to constrain to one with CUDA_VISIBLE_DEVICES=&lt;gpu_to_use&gt;\n",
    "* I used python==3.8; I'm a little hazy but I think I ran into some snag trying 3.9 (my usual)\n",
    "* I think in the training there was not proper eos token handling.  So the model doesn't like to stop so runs on a bit.  I was already half way thru training when I figured this out.... \n",
    "\n",
    "I also included in this repo, my training script (beware faulty eos handling) and a pieced-together generation script.  This notebook is my attempt at cleaning up something for others to consume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU memory usage for inference on a 24G Titan RTX\n",
    "#21763MiB / 24576MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq torch\n",
    "!pip install -Uqq accelerate\n",
    "!pip install -Uqq bitsandbytes\n",
    "!pip install -Uqq git+https://github.com/huggingface/transformers.git\n",
    "#!pip install -Uqq git+https://github.com/sterlind/GPTQ-for-LLaMa.git@lora_4bit\n",
    "#!pip install -Uqq git+https://github.com/sterlind/peft.git\n",
    "!pip install -Uqq sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model file format is changing.  The checkpoints I have are\n",
    "# pre\"v2\" and the code is not backwards compatible\n",
    "# so we need these checkpoints\n",
    "!pip install -Uqq git+https://github.com/sterlind/GPTQ-for-LLaMa.git@d9e903072b507e3d01ced58ccc221641abe14c93\n",
    "!pip install -Uqq git+https://github.com/sterlind/peft.git@ee2ddee858dc1983d5590d939505e60896aa6789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# despite repo name contains 7b,13b,30b,65b 4bit quanitzed llama model weights\n",
    "# This takes a while 29G of files total\n",
    "!git lfs clone https://huggingface.co/maderix/llama-65b-4bit llama_4bit_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/jr/anaconda3/envs/alpaca_lora_30b_4bit_2/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/jr/anaconda3/envs/alpaca_lora_30b_4bit_2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import autograd_4bit\n",
    "from autograd_4bit import load_llama_model_4bit_low_ram, Autograd4bitQuantLinear\n",
    "from peft.tuners.lora import Linear4bitLt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, input=None):\n",
    "    p1 = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\\\n",
    "        \"### Instruction:\\n\"\\\n",
    "        f\"{instruction}\\n\\n\"\n",
    "    p2 = \"### Input:\\n\"\\\n",
    "        f\"{input}\\n\\n\"\n",
    "    p3 = \"### Response:\\n\"\n",
    "\n",
    "    # join the parts\n",
    "    return p1 + (p2 if input else \"\") + p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_llama(*args, **kwargs):\n",
    "    \n",
    "    # quantized (int4) llama base model\n",
    "    model_path = './llama_4bit_quantized/llama30b-4bit.pt'\n",
    "    \n",
    "    # llama base model configuration\n",
    "    config_path = 'decapoda-research/llama-30b-hf'\n",
    "    \n",
    "    # trained lora adapter\n",
    "    lora_path = 'johnrobinsn/alpaca-llama-30b-4bit'    \n",
    "\n",
    "    print(\"Loading {} ...\".format(model_path))\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model, tokenizer = load_llama_model_4bit_low_ram(config_path, model_path)\n",
    "    \n",
    "    model = PeftModel.from_pretrained(model, lora_path, device_map={'': 0}, torch_dtype=torch.float32)\n",
    "    print('{} Lora Applied.'.format(lora_path))\n",
    "    \n",
    "    print('Apply auto switch and half')\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, Autograd4bitQuantLinear) or isinstance(m, Linear4bitLt):\n",
    "            m.zeros = m.zeros.half()\n",
    "            m.scales = m.scales.half()\n",
    "            m.bias = m.bias.half()\n",
    "    autograd_4bit.use_new = True\n",
    "    autograd_4bit.auto_switch = True\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./llama_4bit_quantized/llama30b-4bit.pt ...\n",
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 11.95 seconds.\n",
      "johnrobinsn/alpaca-llama-30b-4bit Lora Applied.\n",
      "Apply auto switch and half\n",
      "Fitting 4bit scales and zeros to half\n"
     ]
    }
   ],
   "source": [
    "model,tokenizer = load_model_llama()\n",
    "\n",
    "print('Fitting 4bit scales and zeros to half')\n",
    "for n, m in model.named_modules():\n",
    "    if '4bit' in str(type(m)):\n",
    "        m.zeros = m.zeros.half()\n",
    "        m.scales = m.scales.half()\n",
    "\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        model, \n",
    "        tokenizer,\n",
    "        instruction,\n",
    "        input=None,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=150,\n",
    "        **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(\n",
    "        instruction, \n",
    "        input if input != \"\" else None\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            early_stopping=True,         \n",
    "        )\n",
    "\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(instruction,input=None):  \n",
    "    output = evaluate(model,tokenizer,\n",
    "        instruction,input)\n",
    "\n",
    "    print(f\"Instruction: {instruction}\\n\")\n",
    "    if (input): print(f\"Input: {input}\\n\")\n",
    "    print(f\"Output: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Write a poem about a cat.\n",
      "\n",
      "Output: A furry feline with a tail so long,\n",
      "Who purrs and purrs and purrs all day long.\n",
      "With whiskers so long and eyes so bright,\n",
      "This kitty is a delight.\n",
      "\n",
      "The paws so soft and the fur so fine,\n",
      "This cat is a joy to behold.\n",
      "With a meow so sweet and a purr so loud,\n",
      "This kitty is a real crowd-pleaser.\n",
      "\n",
      "So if you're looking for a cuddly friend,\n",
      "This cat is the one you should send.\n",
      "With a purr so loud and a meow so sweet,\n",
      "This kitty is the perfect pet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(\"Write a poem about a cat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Who was George Washington? and did he cut down a cherry tree?\n",
      "\n",
      "Output: George Washington was the first President of the United States. He was born on February 22, 1732 in Westmoreland County, Virginia. He served as the Commander-in-Chief of the Continental Army during the American Revolutionary War and was unanimously elected as the first President of the United States in 1789. \n",
      "\n",
      "The story of George Washington chopping down a cherry tree is a legend. It is said that when he was a young boy, he chopped down a cherry tree with his father's hatchet. When his father asked him who did it, he replied, \"I cannot tell a lie, I did it with my hatchet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate('Who was George Washington? and did he cut down a cherry tree?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Identify the odd one out\n",
      "\n",
      "Input: Twitter, Instagram, Telegram\n",
      "\n",
      "Output: Telegram is the odd one out because it is not a social media platform. Twitter and Instagram are both social media platforms. Telegram is a messaging app. Therefore, Telegram is the odd one out. Twitter and Instagram are both social media platforms. Telegram is a messaging app. Therefore, Telegram is the odd one out. Twitter and Instagram are both social media platforms. Telegram is a messaging app. Therefore, Telegram is the odd one out. Twitter and Instagram are both social media platforms. Telegram is a messaging app. Therefore, Telegram is the odd one out. Twitter and Instagram are both social media platforms. Telegram is a messaging app. Therefore, Tele\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example with an instruction + input\n",
    "generate('Identify the odd one out', 'Twitter, Instagram, Telegram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f6904917dc2384a915fcea6693fa257594e766e705fafb79d8450b71bd300453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
